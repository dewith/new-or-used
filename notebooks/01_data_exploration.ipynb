{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to explore the dataset with thousands of Mercado Libre items to see which features are more suitable for our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the packages we need and set the palette for our plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for _ in range(3):\n",
    "    if os.path.exists(f'{os.getcwd()}/setup.py'):\n",
    "        break\n",
    "    os.chdir('..')\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from src.utils.config import get_dataset_path\n",
    "from src.utils.styling import apply_styling, make_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn\n",
    "colors = make_palette()\n",
    "palette = colors['palette']\n",
    "apply_styling(colors)\n",
    "\n",
    "# Pandas\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the jsonl file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = get_dataset_path('raw_items_train')\n",
    "with open(train_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    dict_objects = [json.loads(line) for line in lines[:]]\n",
    "    print('Number of lines:', len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(dict_objects[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print groups of dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {k: type(v) for k, v in dict_objects[0].items()}\n",
    "for dtype in set(dtypes.values()):\n",
    "    print(f'{dtype}:')\n",
    "    for k, v in dtypes.items():\n",
    "        if v == dtype:\n",
    "            print(f'  {k}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(dict_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[target_column].value_counts(dropna=False))\n",
    "print()\n",
    "print(df[target_column].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_column] = df[target_column].map(\n",
    "    {\n",
    "        'new': 0,\n",
    "        'used': 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary columns\n",
    "\n",
    "Preprocess dictionary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location features\n",
    "location_features = ['country', 'state', 'city']\n",
    "for feature in location_features:\n",
    "    df[feature] = df['seller_address'].apply(\n",
    "        lambda x, k=feature: x.get(k, {}).get('name', None)\n",
    "    )\n",
    "\n",
    "df.drop(columns=['seller_address'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shipping features\n",
    "shipping_df = pd.json_normalize(df['shipping'])\n",
    "shipping_df.columns = [\n",
    "    'local_pick_up',\n",
    "    'shipping_methods',\n",
    "    'shipping_tags',\n",
    "    'free_shipping',\n",
    "    'shipping_mode',\n",
    "    'dimensions',\n",
    "    'shipping_free_methods',\n",
    "]\n",
    "\n",
    "df = pd.concat([df.drop('shipping', axis=1), shipping_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = [\n",
    "    'sub_status',\n",
    "    'deal_ids',\n",
    "    'non_mercado_pago_payment_methods',\n",
    "    'variations',\n",
    "    'attributes',\n",
    "    'tags',\n",
    "    'coverage_areas',\n",
    "    'descriptions',\n",
    "    'pictures',\n",
    "]\n",
    "\n",
    "# Replace empty lists with None\n",
    "# for col in list_cols:\n",
    "#     df[col] = df[col].apply(lambda x: np.nan if len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list_cols:\n",
    "    print(col)\n",
    "    try:\n",
    "        values, counts = np.unique(df[col].values, return_counts=True)\n",
    "        print('Unique values:', len(values))\n",
    "        if len(values) < 10:\n",
    "            print('\\n'.join([f'{v}: {c}' for v, c in zip(values, counts)]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_status\n",
    "# Make it a string instead of a list\n",
    "df['sub_status'] = df['sub_status'].apply(lambda x: x[0] if x else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes\n",
    "len_mask = df['attributes'].apply(lambda x: len(x)) > 0\n",
    "attributes_df = pd.json_normalize(df.loc[len_mask, 'attributes'])\n",
    "all_attributes = attributes_df.values.tolist()\n",
    "\n",
    "\n",
    "# Clean attributes\n",
    "def clean_name(name):\n",
    "    \"\"\"Normalize attribute names\"\"\"\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    name = re.sub(r'\\b(de|del)\\s+', '', name, flags=re.IGNORECASE)\n",
    "    name = name.replace(' ', '_').lower().strip()\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return name\n",
    "\n",
    "\n",
    "# Flatten and consolidate attributes\n",
    "attributes = []\n",
    "for row in all_attributes:\n",
    "    for attribute in row:\n",
    "        if attribute:\n",
    "            attribute_name = clean_name(attribute['name'])\n",
    "            value = clean_name(attribute['value_name'])\n",
    "            attributes.append({'attribute': attribute_name, 'value': value})\n",
    "\n",
    "attributes_df = pd.DataFrame(attributes)\n",
    "common_attributes = (attributes_df['attribute'].value_counts() >= 100).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(element: any) -> bool:\n",
    "    # If you expect None to be passed:\n",
    "    if pd.isna(element):\n",
    "        return False\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def consolidate_attributes(attributes):\n",
    "    \"\"\"Consolidate attributes\"\"\"\n",
    "    new_attributes = {}\n",
    "    for attribute in attributes:\n",
    "        attribute_name = clean_name(attribute['name'])\n",
    "        value = clean_name(attribute['value_name'])\n",
    "        if (attribute_name in common_attributes) and ('pieza' not in attribute_name):\n",
    "            new_attributes[f'attr_{attribute_name}'] = value\n",
    "\n",
    "    return new_attributes\n",
    "\n",
    "\n",
    "attributes_dict_df = df['attributes'].apply(consolidate_attributes).to_frame()\n",
    "attributes_normalized_df = pd.json_normalize(attributes_dict_df['attributes'])\n",
    "attributes_dict_df = pd.concat([df[['condition']], attributes_normalized_df], axis=1)\n",
    "attributes_dict_df.fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df):\n",
    "    le = LabelEncoder()\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = le.fit_transform(df[column].astype(str))\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_features(X, y):\n",
    "    # Random Forest Feature Importance\n",
    "    print('Random Forest Feature Importance')\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    rf_scores = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    rf_scores = rf_scores.sort_values(ascending=False)\n",
    "\n",
    "    # ANOVA F-value\n",
    "    print('ANOVA F-value')\n",
    "    f_scores, _ = f_classif(X, y)\n",
    "    f_scores = pd.Series(f_scores, index=X.columns)\n",
    "    f_scores = f_scores.sort_values(ascending=False)\n",
    "    return f_scores, rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dict_encoded_df = encode_categorical(attributes_dict_df)\n",
    "\n",
    "# Split features and target\n",
    "X = attributes_dict_encoded_df.drop(columns=[target_column])\n",
    "y = attributes_dict_encoded_df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scores = {}\n",
    "for i, column in enumerate(X.columns):\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    scores = cross_val_score(model, X[[column]], y, cv=5, scoring='accuracy')\n",
    "    lr_scores[column] = np.mean(scores)\n",
    "    print(f'{i + 1}/{len(X.columns)}: {column} - {lr_scores[column]:.4f}')\n",
    "\n",
    "lr_scores = pd.Series(lr_scores).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores, rf_scores = select_features(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 features for each method\n",
    "print('\\nTop 10 features by ANOVA F-value:\\n', f_scores.head(10))\n",
    "print('\\nTop 10 features by Random Forest Importance:\\n', rf_scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(f_scores, bins=30, color=palette[0])\n",
    "plt.title('ANOVA F-value')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(rf_scores, bins=30, color=palette[1])\n",
    "plt.title('Random Forest Importance')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(lr_scores, bins=30, color=palette[2])\n",
    "plt.title('Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_attributes_anova = f_scores.where(f_scores > 500).dropna().index.tolist()\n",
    "top_attributes_rf = rf_scores.where(rf_scores > 0.01).dropna().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags\n",
    "# Turn tags into flag columns\n",
    "df['tags_str'] = df['tags'].apply(lambda x: ' / '.join(x))\n",
    "tags_dummies = df['tags_str'].str.get_dummies(sep=' / ')\n",
    "df = pd.concat([df, tags_dummies], axis=1)\n",
    "df.drop(columns=['tags', 'tags_str'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage_areas\n",
    "# Drop this column since it is empty\n",
    "df.drop(columns=['coverage_areas'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions\n",
    "# Drop descriptions since they are all unique\n",
    "print(df.descriptions.sample(3))\n",
    "df[['descriptions']].apply(lambda x: x.str.len().value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explored a dataset with millions of Spotify songs and their playlist groupings. You saw which artists and songs are most popular and observed how the distribution of how artists are represented in playlists follows a power law.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
